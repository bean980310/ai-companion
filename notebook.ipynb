{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filename=\"outputs/ComfyUI_00309_.png\"\n",
    "\n",
    "file=os.path.basename(filename)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import json\n",
    "\n",
    "model_id=\"/Users/janghyeonbin/ai-companion/models/llm/gguf/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf\"\n",
    "model=Llama(model_id)\n",
    "\n",
    "chat=\"안녕?\"\n",
    "response = model.create_chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 유용한 AI 비서입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": chat}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoPipelineForText2Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AutoPipelineForText2Image.from_pretrained(\"runwayml/stable-diffusion-v1-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(repo_id=\"faridlazuarda/valadapt-llama-3.1-8B-it-korean\", local_dir=\"./models/llm/loras/valadapt-llama-3.1-8B-it-korean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path=\"/Volumes/EXDATA/models/LLM/transformers/llama/meta-llama__Llama-3.1-8B-Instruct\"\n",
    "lora_path=\"/Users/janghyeonbin/ai-companion/models/llm/loras/valadapt-llama-3.1-8B-it-korean\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "model.load_adapter(lora_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=cv2.imread('./outputs/ComfyUI_00318_.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image=Image.open('ComfyUI_00318_.png').convert('RGBA')\n",
    "original_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=Image.open('layer_0.png').convert('RGBA')\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_alpha=image.getchannel('A')\n",
    "new_alpha.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_alpha=ImageOps.invert(new_alpha)\n",
    "new_alpha.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask=Image.new(\"RGBA\", image.size)\n",
    "new_mask.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mask.putalpha(new_alpha)\n",
    "new_mask.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=os.getenv('HF_TOKEN')\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api=HfApi(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=api.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=api.list_models(task='audio-text-to-text', library=\"transformers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.fftpack\n",
    "import scipy.io.wavfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2mp3():\n",
    "    tts=gTTS('Hello',lang='en')\n",
    "    tts.save('text1.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2mp3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2wav():\n",
    "    tts=gTTS('Hello', lang=\"en\")\n",
    "    tts.save('hello.mp3')\n",
    "    \n",
    "    w=AudioSegment.from_mp3('hello.mp3')\n",
    "    w.export('Hello.wav', format='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2wav()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2wav(text, lang):\n",
    "    tts=gTTS(text, lang=lang)\n",
    "    tts.save(f\"{text}.mp3\")\n",
    "    \n",
    "    w=AudioSegment.from_mp3(f\"{text}.mp3\")\n",
    "    w.export(f\"{text}.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2wav('Hello', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2wav('안녕하세요', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "loader = DirectoryLoader(path='characters_info', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "docs.sort(key=lambda x: x.metadata.get('source', ''))\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain_classic.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document_by_sections(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 [항목] 형태의 섹션을 찾아서 (섹션 제목, 내용) 튜플 리스트로 반환하는 함수.\n",
    "    \"\"\"\n",
    "    # 정규표현식 패턴: 대괄호 안에 항목명, 그 다음 내용은 다음 대괄호가 나오기 전까지\n",
    "    pattern = re.compile(r'\\[(.*?)\\]\\s*(.*?)(?=\\n\\s*\\[|$)', re.DOTALL)\n",
    "    sections = []\n",
    "    for match in pattern.finditer(text):\n",
    "        section_title = match.group(1).strip()\n",
    "        section_content = match.group(2).strip()\n",
    "        sections.append((section_title, section_content))\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어 이름과 코드 매핑 딕셔너리\n",
    "LANGUAGE_MAP = {\n",
    "    \"한국어\": \"ko\",\n",
    "    \"日本語\": \"ja\",\n",
    "    \"简体中文\": \"zh_CN\",\n",
    "    \"繁體中文\": \"zh_TW\",\n",
    "    \"English\": \"en\"\n",
    "}\n",
    "\n",
    "def extract_language_code(heading):\n",
    "    \"\"\"\n",
    "    헤딩 텍스트 (예: \"## 한국어\")에서 언어 이름을 추출하여\n",
    "    매핑 딕셔너리를 통해 언어 코드를 반환하는 함수.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"##\\s*(.+)\", heading)\n",
    "    if match:\n",
    "        lang_name = match.group(1).strip()\n",
    "        return LANGUAGE_MAP.get(lang_name, lang_name)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multilang_document(doc):\n",
    "    \"\"\"\n",
    "    파일 하나에 여러 언어 섹션이 있을 때,\n",
    "    각 언어 헤딩(예: \"## 한국어\")을 기준으로 블록을 분리하고,\n",
    "    각 블록 내에서 [설정], [성격] 등 섹션별로 분리한 Document 리스트를 반환.\n",
    "    \"\"\"\n",
    "    text = doc.page_content\n",
    "    # 언어 헤딩(## 로 시작하는 줄)을 기준으로 분리 (헤딩을 포함하도록 분리)\n",
    "    blocks = re.split(r'(?=##\\s*)', text)\n",
    "    \n",
    "    processed = []\n",
    "    for block in blocks:\n",
    "        if block.startswith(\"##\"):\n",
    "            # 첫 줄에서 언어 헤딩을 추출\n",
    "            lines = block.splitlines()\n",
    "            lang_code = extract_language_code(lines[0])\n",
    "            # 헤딩 부분을 제거한 나머지 내용 사용\n",
    "            content = \"\\n\".join(lines[1:]).strip()\n",
    "            # 이전에 정의한 섹션 분리 함수 사용\n",
    "            sections = split_document_by_sections(content)\n",
    "            for section_title, section_content in sections:\n",
    "                new_doc = Document(\n",
    "                    page_content=section_content,\n",
    "                    metadata={\n",
    "                        \"language\": lang_code,\n",
    "                        \"section\": section_title,\n",
    "                        \"source\": doc.metadata.get(\"source\", \"unknown\")\n",
    "                    }\n",
    "                )\n",
    "                processed.append(new_doc)\n",
    "        else:\n",
    "            # 만약 블록이 언어 헤딩 없이 나온다면, 무시하거나 기본값 설정 가능\n",
    "            pass\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_processed_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc)\n",
    "    processed_docs = process_multilang_document(doc)\n",
    "    all_processed_docs.extend(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in all_processed_docs:\n",
    "    print(doc.metadata, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"BAAI/bge-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model=HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=FAISS.from_documents(\n",
    "    documents=all_processed_docs, embedding=embedding_model, distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_count = vectorstore.index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"미나미 아스카의 성격과 외모에 대해 어떻게 생각해?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embedding_model.embed_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_docs = retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in basic_docs:\n",
    "    print(doc.metadata, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.repetition_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.themes.builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 変換するディレクトリを指定します\n",
    "input_dir = \"/tmp/ComfyUI/models/checkpoints/\"\n",
    "\n",
    "# 指定したディレクトリ内のすべてのsafetensorsファイルを取得\n",
    "safetensors_files = list(Path(input_dir).glob(\"*.safetensors\"))\n",
    "\n",
    "def convert_model(file_path: str):\n",
    "    print(f\"変換中: {file_path}\")\n",
    "    \n",
    "    # メタデータの読み込み\n",
    "    try:\n",
    "        with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            metadata = f.metadata()\n",
    "            metadata = metadata if metadata is not None else {}\n",
    "    except Exception as e:\n",
    "        print(f\"メタデータの読み取り中にエラーが発生しました: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "    # モデルの変換\n",
    "    try:\n",
    "        sd_pruned = {}\n",
    "        with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in tqdm(f.keys(), desc=\"テンソルを変換中\"):\n",
    "                tensor = f.get_tensor(key)\n",
    "                sd_pruned[key] = tensor.to(torch.float8_e4m3fn)\n",
    "\n",
    "        # 変換したモデルを保存（元のディレクトリに）\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        model_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_filename = f\"{model_name}_fp8.safetensors\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        save_file(sd_pruned, output_path, metadata={\"format\": \"pt\", **metadata})\n",
    "        print(f\"ファイルが正常に保存されました: {output_path}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"変換中にエラーが発生しました: {str(e)}\")\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"models/diffusion/checkpoints/sdxl/animagine-xl-4.0-opt.safetensors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_model(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_model_to_int8(file_path: str):\n",
    "    print(f\"Converting: {file_path}\")\n",
    "    \n",
    "    # Read metadata\n",
    "    try:\n",
    "        with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            metadata = f.metadata()\n",
    "            metadata = metadata if metadata is not None else {}\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "    # Convert the model\n",
    "    try:\n",
    "        sd_pruned = {}\n",
    "        with safe_open(file_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for key in tqdm(f.keys(), desc=\"Converting tensors\"):\n",
    "                tensor = f.get_tensor(key)\n",
    "                sd_pruned[key] = tensor.to(torch.float8_e4m3fn)\n",
    "\n",
    "        # Save the converted model (in the original directory)\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "        model_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        output_filename = f\"{model_name}_fp8.safetensors\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        save_file(sd_pruned, output_path, metadata={\"format\": \"pt\", **metadata})\n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {str(e)}\")\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from huggingface_hub import HfApi\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.create_repo(repo_id=\"mlx-community/aya-expanse-8b-8bit\",token=token)\n",
    "api.upload_folder(\n",
    "    folder_path=\"models/llm/mlx/CohereForAI__aya-expanse-8b-8bit\",\n",
    "    repo_id=\"mlx-community/aya-expanse-8b-8bit\",\n",
    "    token=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.upload_folder(\n",
    "    folder_path=\"models/diffusion/loras/sd35_medium/Tomboy_for_SD3.5_Medium_test\",\n",
    "    repo_id=\"bean980310/Tomboy_for_SD3.5_Medium_test\",\n",
    "    ignore_patterns=[\"._*\"],\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"당신은 유용한 AI 비서입니다.\")\n",
    "user_message = HumanMessage(content=\"안녕?\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_message.content),\n",
    "    (\"user\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt.format_messages(input=user_message.content)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.files.create(\n",
    "    file=Path(\"/Users/janghyeonbin/Pictures/ComfyUI_00085_.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import sys\n",
    "import time\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "video = openai.videos.create(\n",
    "    model=\"sora-2-pro\",\n",
    "    input_reference=Path(\"/Users/janghyeonbin/Pictures/ComfyUI_00085_resized.png\"),\n",
    "    seconds=\"8\",\n",
    "    size=\"1280x720\",\n",
    "    prompt=\"Minami Asuka the tomboyish girl is dual wielding swords. her short hair and clothes flutter in the wind. she says 「みんな、俺についてきて！」\",\n",
    ")\n",
    "\n",
    "print(video.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = openai.videos.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = page.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.videos.download_content(\n",
    "    video_id=video.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[Model](data=[Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='gpt-5.1-codex-mini', created=1763007109, object='model', owned_by='system'), Model(id='gpt-5.1-chat-latest', created=1762547951, object='model', owned_by='system'), Model(id='gpt-5.1-2025-11-13', created=1762800353, object='model', owned_by='system'), Model(id='gpt-5.1', created=1762800673, object='model', owned_by='system'), Model(id='gpt-5.1-codex', created=1762988221, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'), Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'), Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system'), Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'), Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system'), Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system'), Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'), Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'), Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system'), Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'), Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'), Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-10-01', created=1727131766, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system'), Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system'), Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system'), Model(id='o1-2024-12-17', created=1734326976, object='model', owned_by='system'), Model(id='o1', created=1734375816, object='model', owned_by='system'), Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system'), Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system'), Model(id='o3-mini', created=1737146383, object='model', owned_by='system'), Model(id='o3-mini-2025-01-31', created=1738010200, object='model', owned_by='system'), Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system'), Model(id='gpt-4o-search-preview-2025-03-11', created=1741388170, object='model', owned_by='system'), Model(id='gpt-4o-search-preview', created=1741388720, object='model', owned_by='system'), Model(id='gpt-4o-mini-search-preview-2025-03-11', created=1741390858, object='model', owned_by='system'), Model(id='gpt-4o-mini-search-preview', created=1741391161, object='model', owned_by='system'), Model(id='gpt-4o-transcribe', created=1742068463, object='model', owned_by='system'), Model(id='gpt-4o-mini-transcribe', created=1742068596, object='model', owned_by='system'), Model(id='o1-pro-2025-03-19', created=1742251504, object='model', owned_by='system'), Model(id='o1-pro', created=1742251791, object='model', owned_by='system'), Model(id='gpt-4o-mini-tts', created=1742403959, object='model', owned_by='system'), Model(id='o3-2025-04-16', created=1744133301, object='model', owned_by='system'), Model(id='o4-mini-2025-04-16', created=1744133506, object='model', owned_by='system'), Model(id='o3', created=1744225308, object='model', owned_by='system'), Model(id='o4-mini', created=1744225351, object='model', owned_by='system'), Model(id='gpt-4.1-2025-04-14', created=1744315746, object='model', owned_by='system'), Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system'), Model(id='gpt-4.1-mini-2025-04-14', created=1744317547, object='model', owned_by='system'), Model(id='gpt-4.1-mini', created=1744318173, object='model', owned_by='system'), Model(id='gpt-4.1-nano-2025-04-14', created=1744321025, object='model', owned_by='system'), Model(id='gpt-4.1-nano', created=1744321707, object='model', owned_by='system'), Model(id='gpt-image-1', created=1745517030, object='model', owned_by='system'), Model(id='codex-mini-latest', created=1746673257, object='model', owned_by='system'), Model(id='o3-pro', created=1748475349, object='model', owned_by='system'), Model(id='gpt-4o-realtime-preview-2025-06-03', created=1748907838, object='model', owned_by='system'), Model(id='gpt-4o-audio-preview-2025-06-03', created=1748908498, object='model', owned_by='system'), Model(id='o3-pro-2025-06-10', created=1749166761, object='model', owned_by='system'), Model(id='o4-mini-deep-research', created=1749685485, object='model', owned_by='system'), Model(id='o3-deep-research', created=1749840121, object='model', owned_by='system'), Model(id='gpt-4o-transcribe-diarize', created=1750798887, object='model', owned_by='system'), Model(id='o3-deep-research-2025-06-26', created=1750865219, object='model', owned_by='system'), Model(id='o4-mini-deep-research-2025-06-26', created=1750866121, object='model', owned_by='system'), Model(id='gpt-5-chat-latest', created=1754073306, object='model', owned_by='system'), Model(id='gpt-5-2025-08-07', created=1754075360, object='model', owned_by='system'), Model(id='gpt-5', created=1754425777, object='model', owned_by='system'), Model(id='gpt-5-mini-2025-08-07', created=1754425867, object='model', owned_by='system'), Model(id='gpt-5-mini', created=1754425928, object='model', owned_by='system'), Model(id='gpt-5-nano-2025-08-07', created=1754426303, object='model', owned_by='system'), Model(id='gpt-5-nano', created=1754426384, object='model', owned_by='system'), Model(id='gpt-audio-2025-08-28', created=1756256146, object='model', owned_by='system'), Model(id='gpt-realtime', created=1756271701, object='model', owned_by='system'), Model(id='gpt-realtime-2025-08-28', created=1756271773, object='model', owned_by='system'), Model(id='gpt-audio', created=1756339249, object='model', owned_by='system'), Model(id='gpt-5-codex', created=1757527818, object='model', owned_by='system'), Model(id='gpt-image-1-mini', created=1758845821, object='model', owned_by='system'), Model(id='gpt-5-pro-2025-10-06', created=1759469707, object='model', owned_by='system'), Model(id='gpt-5-pro', created=1759469822, object='model', owned_by='system'), Model(id='gpt-audio-mini', created=1759512027, object='model', owned_by='system'), Model(id='gpt-audio-mini-2025-10-06', created=1759512137, object='model', owned_by='system'), Model(id='gpt-5-search-api', created=1759514629, object='model', owned_by='system'), Model(id='gpt-realtime-mini', created=1759517133, object='model', owned_by='system'), Model(id='gpt-realtime-mini-2025-10-06', created=1759517175, object='model', owned_by='system'), Model(id='sora-2', created=1759708615, object='model', owned_by='system'), Model(id='sora-2-pro', created=1759708663, object='model', owned_by='system'), Model(id='gpt-5-search-api-2025-10-14', created=1760043960, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')], object='list')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'),\n",
       " Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='gpt-5.1-codex-mini', created=1763007109, object='model', owned_by='system'),\n",
       " Model(id='gpt-5.1-chat-latest', created=1762547951, object='model', owned_by='system'),\n",
       " Model(id='gpt-5.1-2025-11-13', created=1762800353, object='model', owned_by='system'),\n",
       " Model(id='gpt-5.1', created=1762800673, object='model', owned_by='system'),\n",
       " Model(id='gpt-5.1-codex', created=1762988221, object='model', owned_by='system'),\n",
       " Model(id='davinci-002', created=1692634301, object='model', owned_by='system'),\n",
       " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'),\n",
       " Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'),\n",
       " Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'),\n",
       " Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
       " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'),\n",
       " Model(id='gpt-4-0125-preview', created=1706037612, object='model', owned_by='system'),\n",
       " Model(id='gpt-4-turbo-preview', created=1706037777, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'),\n",
       " Model(id='gpt-4-turbo', created=1712361441, object='model', owned_by='system'),\n",
       " Model(id='gpt-4-turbo-2024-04-09', created=1712601677, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o', created=1715367049, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-2024-05-13', created=1715368132, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-2024-07-18', created=1721172717, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini', created=1721172741, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-2024-08-06', created=1722814719, object='model', owned_by='system'),\n",
       " Model(id='chatgpt-4o-latest', created=1723515131, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-realtime-preview-2024-10-01', created=1727131766, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview-2024-10-01', created=1727389042, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview', created=1727460443, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-realtime-preview', created=1727659998, object='model', owned_by='system'),\n",
       " Model(id='omni-moderation-latest', created=1731689265, object='model', owned_by='system'),\n",
       " Model(id='omni-moderation-2024-09-26', created=1732734466, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-realtime-preview-2024-12-17', created=1733945430, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview-2024-12-17', created=1734034239, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-realtime-preview-2024-12-17', created=1734112601, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-audio-preview-2024-12-17', created=1734115920, object='model', owned_by='system'),\n",
       " Model(id='o1-2024-12-17', created=1734326976, object='model', owned_by='system'),\n",
       " Model(id='o1', created=1734375816, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-realtime-preview', created=1734387380, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-audio-preview', created=1734387424, object='model', owned_by='system'),\n",
       " Model(id='o3-mini', created=1737146383, object='model', owned_by='system'),\n",
       " Model(id='o3-mini-2025-01-31', created=1738010200, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-2024-11-20', created=1739331543, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-search-preview-2025-03-11', created=1741388170, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-search-preview', created=1741388720, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-search-preview-2025-03-11', created=1741390858, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-search-preview', created=1741391161, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-transcribe', created=1742068463, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-transcribe', created=1742068596, object='model', owned_by='system'),\n",
       " Model(id='o1-pro-2025-03-19', created=1742251504, object='model', owned_by='system'),\n",
       " Model(id='o1-pro', created=1742251791, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-mini-tts', created=1742403959, object='model', owned_by='system'),\n",
       " Model(id='o3-2025-04-16', created=1744133301, object='model', owned_by='system'),\n",
       " Model(id='o4-mini-2025-04-16', created=1744133506, object='model', owned_by='system'),\n",
       " Model(id='o3', created=1744225308, object='model', owned_by='system'),\n",
       " Model(id='o4-mini', created=1744225351, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-2025-04-14', created=1744315746, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1', created=1744316542, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-mini-2025-04-14', created=1744317547, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-mini', created=1744318173, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-nano-2025-04-14', created=1744321025, object='model', owned_by='system'),\n",
       " Model(id='gpt-4.1-nano', created=1744321707, object='model', owned_by='system'),\n",
       " Model(id='gpt-image-1', created=1745517030, object='model', owned_by='system'),\n",
       " Model(id='codex-mini-latest', created=1746673257, object='model', owned_by='system'),\n",
       " Model(id='o3-pro', created=1748475349, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-realtime-preview-2025-06-03', created=1748907838, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-audio-preview-2025-06-03', created=1748908498, object='model', owned_by='system'),\n",
       " Model(id='o3-pro-2025-06-10', created=1749166761, object='model', owned_by='system'),\n",
       " Model(id='o4-mini-deep-research', created=1749685485, object='model', owned_by='system'),\n",
       " Model(id='o3-deep-research', created=1749840121, object='model', owned_by='system'),\n",
       " Model(id='gpt-4o-transcribe-diarize', created=1750798887, object='model', owned_by='system'),\n",
       " Model(id='o3-deep-research-2025-06-26', created=1750865219, object='model', owned_by='system'),\n",
       " Model(id='o4-mini-deep-research-2025-06-26', created=1750866121, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-chat-latest', created=1754073306, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-2025-08-07', created=1754075360, object='model', owned_by='system'),\n",
       " Model(id='gpt-5', created=1754425777, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-mini-2025-08-07', created=1754425867, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-mini', created=1754425928, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-nano-2025-08-07', created=1754426303, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-nano', created=1754426384, object='model', owned_by='system'),\n",
       " Model(id='gpt-audio-2025-08-28', created=1756256146, object='model', owned_by='system'),\n",
       " Model(id='gpt-realtime', created=1756271701, object='model', owned_by='system'),\n",
       " Model(id='gpt-realtime-2025-08-28', created=1756271773, object='model', owned_by='system'),\n",
       " Model(id='gpt-audio', created=1756339249, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-codex', created=1757527818, object='model', owned_by='system'),\n",
       " Model(id='gpt-image-1-mini', created=1758845821, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-pro-2025-10-06', created=1759469707, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-pro', created=1759469822, object='model', owned_by='system'),\n",
       " Model(id='gpt-audio-mini', created=1759512027, object='model', owned_by='system'),\n",
       " Model(id='gpt-audio-mini-2025-10-06', created=1759512137, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-search-api', created=1759514629, object='model', owned_by='system'),\n",
       " Model(id='gpt-realtime-mini', created=1759517133, object='model', owned_by='system'),\n",
       " Model(id='gpt-realtime-mini-2025-10-06', created=1759517175, object='model', owned_by='system'),\n",
       " Model(id='sora-2', created=1759708615, object='model', owned_by='system'),\n",
       " Model(id='sora-2-pro', created=1759708663, object='model', owned_by='system'),\n",
       " Model(id='gpt-5-search-api-2025-10-14', created=1760043960, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'),\n",
       " Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'),\n",
       " Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'),\n",
       " Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n"
     ]
    }
   ],
   "source": [
    "print(model.data[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0613\n",
      "gpt-4\n",
      "gpt-3.5-turbo\n",
      "gpt-5.1-codex-mini\n",
      "gpt-5.1-chat-latest\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1\n",
      "gpt-5.1-codex\n",
      "davinci-002\n",
      "babbage-002\n",
      "gpt-3.5-turbo-instruct\n",
      "gpt-3.5-turbo-instruct-0914\n",
      "dall-e-3\n",
      "dall-e-2\n",
      "gpt-4-1106-preview\n",
      "gpt-3.5-turbo-1106\n",
      "tts-1-hd\n",
      "tts-1-1106\n",
      "tts-1-hd-1106\n",
      "text-embedding-3-small\n",
      "text-embedding-3-large\n",
      "gpt-4-0125-preview\n",
      "gpt-4-turbo-preview\n",
      "gpt-3.5-turbo-0125\n",
      "gpt-4-turbo\n",
      "gpt-4-turbo-2024-04-09\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-realtime-preview\n",
      "omni-moderation-latest\n",
      "omni-moderation-2024-09-26\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "o1-2024-12-17\n",
      "o1\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "o3-mini\n",
      "o3-mini-2025-01-31\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-mini-transcribe\n",
      "o1-pro-2025-03-19\n",
      "o1-pro\n",
      "gpt-4o-mini-tts\n",
      "o3-2025-04-16\n",
      "o4-mini-2025-04-16\n",
      "o3\n",
      "o4-mini\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-image-1\n",
      "codex-mini-latest\n",
      "o3-pro\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "o3-pro-2025-06-10\n",
      "o4-mini-deep-research\n",
      "o3-deep-research\n",
      "gpt-4o-transcribe-diarize\n",
      "o3-deep-research-2025-06-26\n",
      "o4-mini-deep-research-2025-06-26\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-mini\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-audio-2025-08-28\n",
      "gpt-realtime\n",
      "gpt-realtime-2025-08-28\n",
      "gpt-audio\n",
      "gpt-5-codex\n",
      "gpt-image-1-mini\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-pro\n",
      "gpt-audio-mini\n",
      "gpt-audio-mini-2025-10-06\n",
      "gpt-5-search-api\n",
      "gpt-realtime-mini\n",
      "gpt-realtime-mini-2025-10-06\n",
      "sora-2\n",
      "sora-2-pro\n",
      "gpt-5-search-api-2025-10-14\n",
      "gpt-3.5-turbo-16k\n",
      "tts-1\n",
      "whisper-1\n",
      "text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "for m in model.data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5.1-codex-mini\n",
      "gpt-5.1-chat-latest\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1\n",
      "gpt-5.1-codex\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-mini-2024-07-18\n",
      "gpt-4o-mini\n",
      "gpt-4o-2024-08-06\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-realtime-preview-2024-10-01\n",
      "gpt-4o-audio-preview-2024-10-01\n",
      "gpt-4o-audio-preview\n",
      "gpt-4o-realtime-preview\n",
      "gpt-4o-realtime-preview-2024-12-17\n",
      "gpt-4o-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview-2024-12-17\n",
      "gpt-4o-mini-audio-preview-2024-12-17\n",
      "gpt-4o-mini-realtime-preview\n",
      "gpt-4o-mini-audio-preview\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4o-search-preview-2025-03-11\n",
      "gpt-4o-search-preview\n",
      "gpt-4o-mini-search-preview-2025-03-11\n",
      "gpt-4o-mini-search-preview\n",
      "gpt-4o-transcribe\n",
      "gpt-4o-mini-transcribe\n",
      "gpt-4o-mini-tts\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-4.1-mini-2025-04-14\n",
      "gpt-4.1-mini\n",
      "gpt-4.1-nano-2025-04-14\n",
      "gpt-4.1-nano\n",
      "gpt-4o-realtime-preview-2025-06-03\n",
      "gpt-4o-audio-preview-2025-06-03\n",
      "gpt-4o-transcribe-diarize\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-mini\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-5-codex\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-pro\n",
      "gpt-5-search-api\n",
      "gpt-5-search-api-2025-10-14\n"
     ]
    }
   ],
   "source": [
    "for m in model.data:\n",
    "    if any(k in m.id.lower() for k in [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"]):\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5.1-chat-latest\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-5.1\n",
      "gpt-4o\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-4.1\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-mini\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-nano\n",
      "gpt-5-pro-2025-10-06\n",
      "gpt-5-pro\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "gpt_pattern = [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"]\n",
    "\n",
    "for m in model.data:\n",
    "    model_id = m.id.lower()\n",
    "\n",
    "    date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "    include = any(k in model_id for k in [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"])\n",
    "    exclude_type = all(k not in m.id.lower() for k in [\"image\", \"realtime\", \"tts\", \"audio\", \"transcribe\", \"codex\", \"search\", \"preview\"])\n",
    "    exclude_mini = all(k not in m.id.lower() for k in [\"gpt-4.1-mini\", \"gpt-4.1-nano\", 'gpt-4o-mini'])\n",
    "    latest_or_date = (\"latest\" in model_id) or bool(date_pattern.search(model_id))\n",
    "    if include and exclude_type and exclude_mini:\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-5.1-chat-latest\n",
      "gpt-5.1-2025-11-13\n",
      "gpt-4o-2024-05-13\n",
      "gpt-4o-2024-08-06\n",
      "chatgpt-4o-latest\n",
      "gpt-4o-2024-11-20\n",
      "gpt-4.1-2025-04-14\n",
      "gpt-5-chat-latest\n",
      "gpt-5-2025-08-07\n",
      "gpt-5-mini-2025-08-07\n",
      "gpt-5-nano-2025-08-07\n",
      "gpt-5-pro-2025-10-06\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "gpt_pattern = [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"]\n",
    "\n",
    "for m in model.data:\n",
    "    model_id = m.id.lower()\n",
    "\n",
    "    date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "    include = any(k in model_id for k in [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"])\n",
    "    exclude_type = all(k not in m.id.lower() for k in [\"image\", \"realtime\", \"tts\", \"audio\", \"transcribe\", \"codex\", \"search\", \"preview\"])\n",
    "    exclude_mini = all(k not in m.id.lower() for k in [\"gpt-4.1-mini\", \"gpt-4.1-nano\", 'gpt-4o-mini'])\n",
    "    latest_or_date = (\"latest\" in model_id) or bool(date_pattern.search(model_id))\n",
    "    if include and exclude_type and exclude_mini and latest_or_date:\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.data:\n",
    "    if any(k in m.id.lower() for k in [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"]) and all(k not in m.id.lower() for k in [\"image\", \"realtime\", \"tts\", \"audio\", \"transcribe\", \"codex\", \"search\", \"preview\"]) and all(k not in m.id.lower() for k in [\"gpt-4.1-mini\", \"gpt-4.1-nano\", 'gpt-4o-mini']):\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.data:\n",
    "    if any(k in m.id.lower() for k in [\"dall-e-3\", \"image\"]):\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.data:\n",
    "    if \"tts\" in m.id.lower():\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.data:\n",
    "    if any(k in m.id.lower() for k in [\"whisper\", \"transcribe\"]):\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.data:\n",
    "    if \"text-embedding-3\" in m.id.lower():\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.data:\n",
    "    if any(k in m.id.lower() for k in [\"audio\", \"realtime\"]):\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in model.data:\n",
    "    if \"sora\" in m.id.lower():\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[ModelInfo](data=[ModelInfo(id='claude-opus-4-5-20251101', created_at=datetime.datetime(2025, 11, 24, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4.5', type='model'), ModelInfo(id='claude-haiku-4-5-20251001', created_at=datetime.datetime(2025, 10, 15, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 4.5', type='model'), ModelInfo(id='claude-sonnet-4-5-20250929', created_at=datetime.datetime(2025, 9, 29, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 4.5', type='model'), ModelInfo(id='claude-opus-4-1-20250805', created_at=datetime.datetime(2025, 8, 5, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4.1', type='model'), ModelInfo(id='claude-opus-4-20250514', created_at=datetime.datetime(2025, 5, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4', type='model'), ModelInfo(id='claude-sonnet-4-20250514', created_at=datetime.datetime(2025, 5, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 4', type='model'), ModelInfo(id='claude-3-7-sonnet-20250219', created_at=datetime.datetime(2025, 2, 24, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 3.7', type='model'), ModelInfo(id='claude-3-5-haiku-20241022', created_at=datetime.datetime(2024, 10, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 3.5', type='model'), ModelInfo(id='claude-3-haiku-20240307', created_at=datetime.datetime(2024, 3, 7, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 3', type='model'), ModelInfo(id='claude-3-opus-20240229', created_at=datetime.datetime(2024, 2, 29, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 3', type='model')], has_more=False, first_id='claude-opus-4-5-20251101', last_id='claude-3-opus-20240229')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelInfo(id='claude-opus-4-5-20251101', created_at=datetime.datetime(2025, 11, 24, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4.5', type='model'),\n",
       " ModelInfo(id='claude-haiku-4-5-20251001', created_at=datetime.datetime(2025, 10, 15, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 4.5', type='model'),\n",
       " ModelInfo(id='claude-sonnet-4-5-20250929', created_at=datetime.datetime(2025, 9, 29, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 4.5', type='model'),\n",
       " ModelInfo(id='claude-opus-4-1-20250805', created_at=datetime.datetime(2025, 8, 5, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4.1', type='model'),\n",
       " ModelInfo(id='claude-opus-4-20250514', created_at=datetime.datetime(2025, 5, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 4', type='model'),\n",
       " ModelInfo(id='claude-sonnet-4-20250514', created_at=datetime.datetime(2025, 5, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 4', type='model'),\n",
       " ModelInfo(id='claude-3-7-sonnet-20250219', created_at=datetime.datetime(2025, 2, 24, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Sonnet 3.7', type='model'),\n",
       " ModelInfo(id='claude-3-5-haiku-20241022', created_at=datetime.datetime(2024, 10, 22, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 3.5', type='model'),\n",
       " ModelInfo(id='claude-3-haiku-20240307', created_at=datetime.datetime(2024, 3, 7, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Haiku 3', type='model'),\n",
       " ModelInfo(id='claude-3-opus-20240229', created_at=datetime.datetime(2024, 2, 29, 0, 0, tzinfo=datetime.timezone.utc), display_name='Claude Opus 3', type='model')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in client.models.list().data:\n",
    "    print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-opus-4-5-20251101\n",
      "claude-haiku-4-5-20251001\n",
      "claude-sonnet-4-5-20250929\n",
      "claude-opus-4-1-20250805\n",
      "claude-opus-4-20250514\n",
      "claude-sonnet-4-20250514\n"
     ]
    }
   ],
   "source": [
    "for m in client.models.list().data:\n",
    "    exclude = \"claude-3\" not in m.id.lower()\n",
    "    if exclude:\n",
    "        print(m.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(http_options={'api_version': 'v1beta'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Embedding Gecko',\n",
       "   input_token_limit=1024,\n",
       "   name='models/embedding-gecko-001',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedText',\n",
       "     'countTextTokens',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (Nov 25th, 2025) of Gemini 2.5 Pro with VTEA and DA CSI',\n",
       "   display_name='Gemini 2.5 Pro Preview with VTEA and DA CSI',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-vtea-da-csi',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-pro-vtea-da-csi'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Pro Preview 03-25',\n",
       "   display_name='Gemini 2.5 Pro Preview 03-25',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-03-25',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.',\n",
       "   display_name='Gemini 2.5 Flash',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.5 Pro Preview 05-06',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-05-06',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-06'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.5 Pro Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-06-05',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-06-05'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.5 Pro',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash Experimental',\n",
       "   display_name='Gemini 2.0 Flash Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-exp',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'bidiGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash',\n",
       "   display_name='Gemini 2.0 Flash',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.',\n",
       "   display_name='Gemini 2.0 Flash 001',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-001',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "   display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-exp-image-generation',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'bidiGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite 001',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite-001',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='preview-02-05'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite-preview',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='preview-02-05'\n",
       " ),\n",
       " Model(\n",
       "   description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.0 Pro Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-pro-exp',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-exp-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.0 Pro Experimental 02-05',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-pro-exp-02-05',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-exp-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini Experimental 1206',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-exp-1206',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-exp-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "   display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-20'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "   display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-thinking-exp',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-20'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "   display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-thinking-exp-1219',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-20'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview TTS',\n",
       "   display_name='Gemini 2.5 Flash Preview TTS',\n",
       "   input_token_limit=8192,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-preview-tts',\n",
       "   output_token_limit=16384,\n",
       "   supported_actions=[\n",
       "     'countTokens',\n",
       "     'generateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='gemini-2.5-flash-exp-tts-2025-05-19'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Pro Preview TTS',\n",
       "   display_name='Gemini 2.5 Pro Preview TTS',\n",
       "   input_token_limit=8192,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-tts',\n",
       "   output_token_limit=16384,\n",
       "   supported_actions=[\n",
       "     'countTokens',\n",
       "     'generateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='gemini-2.5-pro-preview-tts-2025-05-19'\n",
       " ),\n",
       " Model(\n",
       "   description='LearnLM 2.0 Flash Experimental',\n",
       "   display_name='LearnLM 2.0 Flash Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/learnlm-2.0-flash-experimental',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 1B',\n",
       "   input_token_limit=32768,\n",
       "   name='models/gemma-3-1b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 4B',\n",
       "   input_token_limit=32768,\n",
       "   name='models/gemma-3-4b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 12B',\n",
       "   input_token_limit=32768,\n",
       "   name='models/gemma-3-12b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 27B',\n",
       "   input_token_limit=131072,\n",
       "   name='models/gemma-3-27b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3n E4B',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemma-3n-e4b-it',\n",
       "   output_token_limit=2048,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3n E2B',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemma-3n-e2b-it',\n",
       "   output_token_limit=2048,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Latest release of Gemini Flash',\n",
       "   display_name='Gemini Flash Latest',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-flash-latest',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini Flash Latest'\n",
       " ),\n",
       " Model(\n",
       "   description='Latest release of Gemini Flash-Lite',\n",
       "   display_name='Gemini Flash-Lite Latest',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-flash-lite-latest',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini Flash-Lite Latest'\n",
       " ),\n",
       " Model(\n",
       "   description='Latest release of Gemini Pro',\n",
       "   display_name='Gemini Pro Latest',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-pro-latest',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini Pro Latest'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
       "   display_name='Gemini 2.5 Flash-Lite',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-lite',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview Image',\n",
       "   display_name='Nano Banana',\n",
       "   input_token_limit=32768,\n",
       "   max_temperature=1.0,\n",
       "   name='models/gemini-2.5-flash-image-preview',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview Image',\n",
       "   display_name='Nano Banana',\n",
       "   input_token_limit=32768,\n",
       "   max_temperature=1.0,\n",
       "   name='models/gemini-2.5-flash-image',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview Sep 2025',\n",
       "   display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-preview-09-2025',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini 2.5 Flash Preview 09-2025'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "   display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-09-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 3 Pro Preview',\n",
       "   display_name='Gemini 3 Pro Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-3-pro-preview',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='3-pro-preview-11-2025'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 3 Pro Image Preview',\n",
       "   display_name='Nano Banana Pro',\n",
       "   input_token_limit=131072,\n",
       "   max_temperature=1.0,\n",
       "   name='models/gemini-3-pro-image-preview',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='3.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 3 Pro Image Preview',\n",
       "   display_name='Nano Banana Pro',\n",
       "   input_token_limit=131072,\n",
       "   max_temperature=1.0,\n",
       "   name='models/nano-banana-pro-preview',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='3.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini Robotics-ER 1.5 Preview',\n",
       "   display_name='Gemini Robotics-ER 1.5 Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-robotics-er-1.5-preview',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='1.5-preview'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "   display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "   input_token_limit=131072,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-computer-use-preview-10-2025',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini 2.5 Computer Use Preview 10-2025'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Embedding 001',\n",
       "   input_token_limit=2048,\n",
       "   name='models/embedding-001',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Text Embedding 004',\n",
       "   input_token_limit=2048,\n",
       "   name='models/text-embedding-004',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='004'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Gemini Embedding Experimental 03-07',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemini-embedding-exp-03-07',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "     'countTextTokens',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='exp-03-07'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Gemini Embedding Experimental',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemini-embedding-exp',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "     'countTextTokens',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='exp-03-07'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Gemini Embedding 001',\n",
       "   input_token_limit=2048,\n",
       "   name='models/gemini-embedding-001',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "     'countTextTokens',\n",
       "     'countTokens',\n",
       "     'asyncBatchEmbedContent',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.',\n",
       "   display_name='Model that performs Attributed Question Answering.',\n",
       "   input_token_limit=7168,\n",
       "   name='models/aqa',\n",
       "   output_token_limit=1024,\n",
       "   supported_actions=[\n",
       "     'generateAnswer',\n",
       "   ],\n",
       "   temperature=0.2,\n",
       "   top_k=40,\n",
       "   top_p=1.0,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Vertex served Imagen 4.0 model',\n",
       "   display_name='Imagen 4 (Preview)',\n",
       "   input_token_limit=480,\n",
       "   name='models/imagen-4.0-generate-preview-06-06',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'predict',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='01'\n",
       " )]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Embedding Gecko'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.models.list().page[0].display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Embedding Gecko',\n",
       "   input_token_limit=1024,\n",
       "   name='models/embedding-gecko-001',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedText',\n",
       "     'countTextTokens',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (Nov 25th, 2025) of Gemini 2.5 Pro with VTEA and DA CSI',\n",
       "   display_name='Gemini 2.5 Pro Preview with VTEA and DA CSI',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-vtea-da-csi',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-pro-vtea-da-csi'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Pro Preview 03-25',\n",
       "   display_name='Gemini 2.5 Pro Preview 03-25',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-03-25',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.',\n",
       "   display_name='Gemini 2.5 Flash',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (May 6th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.5 Pro Preview 05-06',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-05-06',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-06'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (June 5th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.5 Pro Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-06-05',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-06-05'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable release (June 17th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.5 Pro',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash Experimental',\n",
       "   display_name='Gemini 2.0 Flash Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-exp',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'bidiGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash',\n",
       "   display_name='Gemini 2.0 Flash',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.',\n",
       "   display_name='Gemini 2.0 Flash 001',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-001',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "   display_name='Gemini 2.0 Flash (Image Generation) Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-exp-image-generation',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'bidiGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite 001',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite-001',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite Preview 02-05',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite-preview-02-05',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='preview-02-05'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite',\n",
       "   display_name='Gemini 2.0 Flash-Lite Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-lite-preview',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=40,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='preview-02-05'\n",
       " ),\n",
       " Model(\n",
       "   description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.0 Pro Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-pro-exp',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-exp-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini 2.0 Pro Experimental 02-05',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-pro-exp-02-05',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-exp-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro',\n",
       "   display_name='Gemini Experimental 1206',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-exp-1206',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-exp-03-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "   display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-thinking-exp-01-21',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-20'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "   display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-thinking-exp',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-20'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (April 17th, 2025) of Gemini 2.5 Flash',\n",
       "   display_name='Gemini 2.5 Flash Preview 05-20',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.0-flash-thinking-exp-1219',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-05-20'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview TTS',\n",
       "   display_name='Gemini 2.5 Flash Preview TTS',\n",
       "   input_token_limit=8192,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-preview-tts',\n",
       "   output_token_limit=16384,\n",
       "   supported_actions=[\n",
       "     'countTokens',\n",
       "     'generateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='gemini-2.5-flash-exp-tts-2025-05-19'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Pro Preview TTS',\n",
       "   display_name='Gemini 2.5 Pro Preview TTS',\n",
       "   input_token_limit=8192,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-pro-preview-tts',\n",
       "   output_token_limit=16384,\n",
       "   supported_actions=[\n",
       "     'countTokens',\n",
       "     'generateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='gemini-2.5-pro-preview-tts-2025-05-19'\n",
       " ),\n",
       " Model(\n",
       "   description='LearnLM 2.0 Flash Experimental',\n",
       "   display_name='LearnLM 2.0 Flash Experimental',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/learnlm-2.0-flash-experimental',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 1B',\n",
       "   input_token_limit=32768,\n",
       "   name='models/gemma-3-1b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 4B',\n",
       "   input_token_limit=32768,\n",
       "   name='models/gemma-3-4b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 12B',\n",
       "   input_token_limit=32768,\n",
       "   name='models/gemma-3-12b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3 27B',\n",
       "   input_token_limit=131072,\n",
       "   name='models/gemma-3-27b-it',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3n E4B',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemma-3n-e4b-it',\n",
       "   output_token_limit=2048,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   display_name='Gemma 3n E2B',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemma-3n-e2b-it',\n",
       "   output_token_limit=2048,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Latest release of Gemini Flash',\n",
       "   display_name='Gemini Flash Latest',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-flash-latest',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini Flash Latest'\n",
       " ),\n",
       " Model(\n",
       "   description='Latest release of Gemini Flash-Lite',\n",
       "   display_name='Gemini Flash-Lite Latest',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-flash-lite-latest',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini Flash-Lite Latest'\n",
       " ),\n",
       " Model(\n",
       "   description='Latest release of Gemini Pro',\n",
       "   display_name='Gemini Pro Latest',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-pro-latest',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini Pro Latest'\n",
       " ),\n",
       " Model(\n",
       "   description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025',\n",
       "   display_name='Gemini 2.5 Flash-Lite',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-lite',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview Image',\n",
       "   display_name='Nano Banana',\n",
       "   input_token_limit=32768,\n",
       "   max_temperature=1.0,\n",
       "   name='models/gemini-2.5-flash-image-preview',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview Image',\n",
       "   display_name='Nano Banana',\n",
       "   input_token_limit=32768,\n",
       "   max_temperature=1.0,\n",
       "   name='models/gemini-2.5-flash-image',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Flash Preview Sep 2025',\n",
       "   display_name='Gemini 2.5 Flash Preview Sep 2025',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-preview-09-2025',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini 2.5 Flash Preview 09-2025'\n",
       " ),\n",
       " Model(\n",
       "   description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite',\n",
       "   display_name='Gemini 2.5 Flash-Lite Preview Sep 2025',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-flash-lite-preview-09-2025',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='2.5-preview-09-25'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 3 Pro Preview',\n",
       "   display_name='Gemini 3 Pro Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-3-pro-preview',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'createCachedContent',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='3-pro-preview-11-2025'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 3 Pro Image Preview',\n",
       "   display_name='Nano Banana Pro',\n",
       "   input_token_limit=131072,\n",
       "   max_temperature=1.0,\n",
       "   name='models/gemini-3-pro-image-preview',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='3.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 3 Pro Image Preview',\n",
       "   display_name='Nano Banana Pro',\n",
       "   input_token_limit=131072,\n",
       "   max_temperature=1.0,\n",
       "   name='models/nano-banana-pro-preview',\n",
       "   output_token_limit=32768,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "     'batchGenerateContent',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='3.0'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini Robotics-ER 1.5 Preview',\n",
       "   display_name='Gemini Robotics-ER 1.5 Preview',\n",
       "   input_token_limit=1048576,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-robotics-er-1.5-preview',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='1.5-preview'\n",
       " ),\n",
       " Model(\n",
       "   description='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "   display_name='Gemini 2.5 Computer Use Preview 10-2025',\n",
       "   input_token_limit=131072,\n",
       "   max_temperature=2.0,\n",
       "   name='models/gemini-2.5-computer-use-preview-10-2025',\n",
       "   output_token_limit=65536,\n",
       "   supported_actions=[\n",
       "     'generateContent',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   temperature=1.0,\n",
       "   thinking=True,\n",
       "   top_k=64,\n",
       "   top_p=0.95,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='Gemini 2.5 Computer Use Preview 10-2025'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Embedding 001',\n",
       "   input_token_limit=2048,\n",
       "   name='models/embedding-001',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Text Embedding 004',\n",
       "   input_token_limit=2048,\n",
       "   name='models/text-embedding-004',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='004'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Gemini Embedding Experimental 03-07',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemini-embedding-exp-03-07',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "     'countTextTokens',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='exp-03-07'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Gemini Embedding Experimental',\n",
       "   input_token_limit=8192,\n",
       "   name='models/gemini-embedding-exp',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "     'countTextTokens',\n",
       "     'countTokens',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='exp-03-07'\n",
       " ),\n",
       " Model(\n",
       "   description='Obtain a distributed representation of a text.',\n",
       "   display_name='Gemini Embedding 001',\n",
       "   input_token_limit=2048,\n",
       "   name='models/gemini-embedding-001',\n",
       "   output_token_limit=1,\n",
       "   supported_actions=[\n",
       "     'embedContent',\n",
       "     'countTextTokens',\n",
       "     'countTokens',\n",
       "     'asyncBatchEmbedContent',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.',\n",
       "   display_name='Model that performs Attributed Question Answering.',\n",
       "   input_token_limit=7168,\n",
       "   name='models/aqa',\n",
       "   output_token_limit=1024,\n",
       "   supported_actions=[\n",
       "     'generateAnswer',\n",
       "   ],\n",
       "   temperature=0.2,\n",
       "   top_k=40,\n",
       "   top_p=1.0,\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='001'\n",
       " ),\n",
       " Model(\n",
       "   description='Vertex served Imagen 4.0 model',\n",
       "   display_name='Imagen 4 (Preview)',\n",
       "   input_token_limit=480,\n",
       "   name='models/imagen-4.0-generate-preview-06-06',\n",
       "   output_token_limit=8192,\n",
       "   supported_actions=[\n",
       "     'predict',\n",
       "   ],\n",
       "   tuned_model_info=TunedModelInfo(),\n",
       "   version='01'\n",
       " )]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for m in response.page:\n",
    "    models.append(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = []\n",
    "for m in response.page:\n",
    "    if \"generateContent\" in m.supported_actions:\n",
    "        llm.append(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gemini-2.5-pro-vtea-da-csi',\n",
       " 'models/gemini-2.5-pro-preview-03-25',\n",
       " 'models/gemini-2.5-flash',\n",
       " 'models/gemini-2.5-pro-preview-05-06',\n",
       " 'models/gemini-2.5-pro-preview-06-05',\n",
       " 'models/gemini-2.5-pro',\n",
       " 'models/gemini-2.0-flash-exp',\n",
       " 'models/gemini-2.0-flash',\n",
       " 'models/gemini-2.0-flash-001',\n",
       " 'models/gemini-2.0-flash-exp-image-generation',\n",
       " 'models/gemini-2.0-flash-lite-001',\n",
       " 'models/gemini-2.0-flash-lite',\n",
       " 'models/gemini-2.0-flash-lite-preview-02-05',\n",
       " 'models/gemini-2.0-flash-lite-preview',\n",
       " 'models/gemini-2.0-pro-exp',\n",
       " 'models/gemini-2.0-pro-exp-02-05',\n",
       " 'models/gemini-exp-1206',\n",
       " 'models/gemini-2.0-flash-thinking-exp-01-21',\n",
       " 'models/gemini-2.0-flash-thinking-exp',\n",
       " 'models/gemini-2.0-flash-thinking-exp-1219',\n",
       " 'models/gemini-2.5-flash-preview-tts',\n",
       " 'models/gemini-2.5-pro-preview-tts',\n",
       " 'models/learnlm-2.0-flash-experimental',\n",
       " 'models/gemma-3-1b-it',\n",
       " 'models/gemma-3-4b-it',\n",
       " 'models/gemma-3-12b-it',\n",
       " 'models/gemma-3-27b-it',\n",
       " 'models/gemma-3n-e4b-it',\n",
       " 'models/gemma-3n-e2b-it',\n",
       " 'models/gemini-flash-latest',\n",
       " 'models/gemini-flash-lite-latest',\n",
       " 'models/gemini-pro-latest',\n",
       " 'models/gemini-2.5-flash-lite',\n",
       " 'models/gemini-2.5-flash-image-preview',\n",
       " 'models/gemini-2.5-flash-image',\n",
       " 'models/gemini-2.5-flash-preview-09-2025',\n",
       " 'models/gemini-2.5-flash-lite-preview-09-2025',\n",
       " 'models/gemini-3-pro-preview',\n",
       " 'models/gemini-3-pro-image-preview',\n",
       " 'models/nano-banana-pro-preview',\n",
       " 'models/gemini-robotics-er-1.5-preview',\n",
       " 'models/gemini-2.5-computer-use-preview-10-2025']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/embedding-gecko-001',\n",
       " 'models/gemini-2.5-pro-vtea-da-csi',\n",
       " 'models/gemini-2.5-pro-preview-03-25',\n",
       " 'models/gemini-2.5-flash',\n",
       " 'models/gemini-2.5-pro-preview-05-06',\n",
       " 'models/gemini-2.5-pro-preview-06-05',\n",
       " 'models/gemini-2.5-pro',\n",
       " 'models/gemini-2.0-flash-exp',\n",
       " 'models/gemini-2.0-flash',\n",
       " 'models/gemini-2.0-flash-001',\n",
       " 'models/gemini-2.0-flash-exp-image-generation',\n",
       " 'models/gemini-2.0-flash-lite-001',\n",
       " 'models/gemini-2.0-flash-lite',\n",
       " 'models/gemini-2.0-flash-lite-preview-02-05',\n",
       " 'models/gemini-2.0-flash-lite-preview',\n",
       " 'models/gemini-2.0-pro-exp',\n",
       " 'models/gemini-2.0-pro-exp-02-05',\n",
       " 'models/gemini-exp-1206',\n",
       " 'models/gemini-2.0-flash-thinking-exp-01-21',\n",
       " 'models/gemini-2.0-flash-thinking-exp',\n",
       " 'models/gemini-2.0-flash-thinking-exp-1219',\n",
       " 'models/gemini-2.5-flash-preview-tts',\n",
       " 'models/gemini-2.5-pro-preview-tts',\n",
       " 'models/learnlm-2.0-flash-experimental',\n",
       " 'models/gemma-3-1b-it',\n",
       " 'models/gemma-3-4b-it',\n",
       " 'models/gemma-3-12b-it',\n",
       " 'models/gemma-3-27b-it',\n",
       " 'models/gemma-3n-e4b-it',\n",
       " 'models/gemma-3n-e2b-it',\n",
       " 'models/gemini-flash-latest',\n",
       " 'models/gemini-flash-lite-latest',\n",
       " 'models/gemini-pro-latest',\n",
       " 'models/gemini-2.5-flash-lite',\n",
       " 'models/gemini-2.5-flash-image-preview',\n",
       " 'models/gemini-2.5-flash-image',\n",
       " 'models/gemini-2.5-flash-preview-09-2025',\n",
       " 'models/gemini-2.5-flash-lite-preview-09-2025',\n",
       " 'models/gemini-3-pro-preview',\n",
       " 'models/gemini-3-pro-image-preview',\n",
       " 'models/nano-banana-pro-preview',\n",
       " 'models/gemini-robotics-er-1.5-preview',\n",
       " 'models/gemini-2.5-computer-use-preview-10-2025',\n",
       " 'models/embedding-001',\n",
       " 'models/text-embedding-004',\n",
       " 'models/gemini-embedding-exp-03-07',\n",
       " 'models/gemini-embedding-exp',\n",
       " 'models/gemini-embedding-001',\n",
       " 'models/aqa',\n",
       " 'models/imagen-4.0-generate-preview-06-06']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemini-2.5-pro-vtea-da-csi',\n",
       " 'gemini-2.5-flash',\n",
       " 'gemini-2.5-pro',\n",
       " 'gemma-3-1b-it',\n",
       " 'gemma-3-4b-it',\n",
       " 'gemma-3-12b-it',\n",
       " 'gemma-3-27b-it',\n",
       " 'gemma-3n-e4b-it',\n",
       " 'gemma-3n-e2b-it',\n",
       " 'gemini-flash-latest',\n",
       " 'gemini-flash-lite-latest',\n",
       " 'gemini-pro-latest',\n",
       " 'gemini-2.5-flash-lite',\n",
       " 'gemini-2.5-flash-image',\n",
       " 'gemini-3-pro-preview',\n",
       " 'gemini-3-pro-image-preview']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = []\n",
    "for m in response.page:\n",
    "    include = any(k in m.name.lower() for k in [\"gemini\", \"gemma\"])\n",
    "    exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3-pro-preview\", \"gemini-3-pro-image-preview\"])\n",
    "    if \"generateContent\" in m.supported_actions and include and exclude_type and exclude_model and preview_check:\n",
    "        llm.append(m.name.split('/')[-1])\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/imagen-4.0-generate-preview-06-06']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model = []\n",
    "for m in response.page:\n",
    "    if \"imagen\" in m.name.lower():\n",
    "        image_model.append(m.name)\n",
    "\n",
    "image_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import get_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perplexity\n",
    "from perplexity import Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Perplexity(api_key=get_key(dotenv_path=\".env\", key_to_get=\"PERPLEXITY_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    test = client.search.create(query=\"test\")\n",
    "except perplexity.AuthenticationError:\n",
    "    print(\"Invalid API key. Please check your PERPLEXITY_API_KEY environment variable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xai_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = xai_sdk.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = client.models.list_language_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = lms.list_downloaded_models(\"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in llm:\n",
    "    print(m.model_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models.models:\n",
    "    print(m.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_address=\"127.0.0.1:8000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_route = f\"http://{server_address}/models/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = requests.get(model_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\"ilxl/Illustrious-XL-v1.0.safetensors\", \"sd15/AOM3A1B_orangemixs.safetensors\", \"sd15/Anything-ink.safetensors\", \"sd15/AnythingXL_v50.safetensors\", \"sd15/Counterfeit-V3.0_fix_fp16.safetensors\", \"sd15/anythingV3_fp16.safetensors\", \"sd15/sd-v1-5-inpainting.ckpt\", \"sd15/v1-5-pruned-emaonly.safetensors\", \"sd35_large/emi3.safetensors\", \"sdxl/AnythingXL_xl.safetensors\", \"sdxl/AnythingXL_xlHyper8stepsCFG1.safetensors\", \"sdxl/animagine-xl-3.1.safetensors\", \"sdxl/animagine-xl-4.0-opt.safetensors\", \"sdxl/fp8/animagine-xl-4.0-opt-fp8.safetensors\", \"sdxl/sd_xl_base_1.0.safetensors\", \"sdxl/sd_xl_refiner_1.0.safetensors\"'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.content[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_byte = models.content[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = models.content[1:-1].decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"ilxl/Illustrious-XL-v1.0.safetensors\", \"sd15/AOM3A1B_orangemixs.safetensors\", \"sd15/Anything-ink.safetensors\", \"sd15/AnythingXL_v50.safetensors\", \"sd15/Counterfeit-V3.0_fix_fp16.safetensors\", \"sd15/anythingV3_fp16.safetensors\", \"sd15/sd-v1-5-inpainting.ckpt\", \"sd15/v1-5-pruned-emaonly.safetensors\", \"sd35_large/emi3.safetensors\", \"sdxl/AnythingXL_xl.safetensors\", \"sdxl/AnythingXL_xlHyper8stepsCFG1.safetensors\", \"sdxl/animagine-xl-3.1.safetensors\", \"sdxl/animagine-xl-4.0-opt.safetensors\", \"sdxl/fp8/animagine-xl-4.0-opt-fp8.safetensors\", \"sdxl/sd_xl_base_1.0.safetensors\", \"sdxl/sd_xl_refiner_1.0.safetensors\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_list = [x.decode('utf-8') for x in checkpoints_byte.split(b', ') if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoints)\n",
    "print(type(checkpoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_list = [x for x in checkpoints.replace(\"\\\"\", \"\").strip().split(', ') if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ilxl/Illustrious-XL-v1.0.safetensors',\n",
       " 'sd15/AOM3A1B_orangemixs.safetensors',\n",
       " 'sd15/Anything-ink.safetensors',\n",
       " 'sd15/AnythingXL_v50.safetensors',\n",
       " 'sd15/Counterfeit-V3.0_fix_fp16.safetensors',\n",
       " 'sd15/anythingV3_fp16.safetensors',\n",
       " 'sd15/sd-v1-5-inpainting.ckpt',\n",
       " 'sd15/v1-5-pruned-emaonly.safetensors',\n",
       " 'sd35_large/emi3.safetensors',\n",
       " 'sdxl/AnythingXL_xl.safetensors',\n",
       " 'sdxl/AnythingXL_xlHyper8stepsCFG1.safetensors',\n",
       " 'sdxl/animagine-xl-3.1.safetensors',\n",
       " 'sdxl/animagine-xl-4.0-opt.safetensors',\n",
       " 'sdxl/fp8/animagine-xl-4.0-opt-fp8.safetensors',\n",
       " 'sdxl/sd_xl_base_1.0.safetensors',\n",
       " 'sdxl/sd_xl_refiner_1.0.safetensors']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in checkpoint_list:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ComfyUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are a few options ranging from direct to slightly more conversational. Choose the one that best fits your relationship with your boss.\n",
      "\n",
      "**Option 1: Clear\n",
      " and direct (Best for most situations)**\n",
      "\n",
      "**Subject:** WFH Request for Tomorrow, [insert date here] – [Your Name]\n",
      "\n",
      "Hi [Boss's\n",
      " Name],\n",
      "\n",
      "I would like to request to work from home tomorrow, [insert day, e.g., Tuesday].\n",
      "\n",
      "I need to be present at my apartment to let\n",
      " in a repairman for some necessary maintenance during business hours.\n",
      "\n",
      "Please be assured that I will be fully connected with stable internet and reachable via email and chat as usual. I\n",
      " do not anticipate any disruption to my workflow.\n",
      "\n",
      "Please let me know if this works for you.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "***\n",
      "\n",
      "**Option \n",
      "2: Slightly softer tone (If you have a casual rapport)**\n",
      "\n",
      "**Subject:** Working from home tomorrow - [Your Name]\n",
      "\n",
      "Hi [Boss's\n",
      " Name],\n",
      "\n",
      "Would it be okay if I logged in from home tomorrow, [insert day]?\n",
      "\n",
      "I have a repairman scheduled to come by my place during the day,\n",
      " and I need to be here to let them in.\n",
      "\n",
      "My home setup is ready to go, so I’ll be fully online and working my normal hours without interruption\n",
      ".\n",
      "\n",
      "Thanks for understanding,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "***\n",
      "\n",
      "**Option 3: Short and specific (If your boss prefers brevity)**\n",
      "\n",
      "**Subject:** WFH\n",
      " tomorrow, [Date] due to apartment repairs\n",
      "\n",
      "Hi [Boss's Name],\n",
      "\n",
      "I’m writing to request to work from home tomorrow to accommodate a necessary\n",
      " apartment repair appointment.\n",
      "\n",
      "I will be fully reachable and working normally throughout the day.\n",
      "\n",
      "Please let me know if this is approved.\n",
      "\n",
      "Thanks,\n",
      "\n",
      "[Your Name\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# To run this code you need to install the following dependencies:\n",
    "# pip install google-genai\n",
    "\n",
    "import base64\n",
    "import mimetypes\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "def save_binary_file(file_name, data):\n",
    "    f = open(file_name, \"wb\")\n",
    "    f.write(data)\n",
    "    f.close()\n",
    "    print(f\"File saved to to: {file_name}\")\n",
    "\n",
    "\n",
    "def generate():\n",
    "    client = genai.Client(\n",
    "        api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    model = \"gemini-3-pro-image-preview\"\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[\n",
    "                types.Part.from_text(text=\"\"\"INSERT_INPUT_HERE\"\"\"),\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    tools = [\n",
    "        types.Tool(googleSearch=types.GoogleSearch(\n",
    "        )),\n",
    "    ]\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        response_modalities=[\n",
    "            \"IMAGE\",\n",
    "            \"TEXT\",\n",
    "        ],\n",
    "        image_config=types.ImageConfig(\n",
    "            aspect_ratio=\"16:9\",\n",
    "            image_size=\"4K\",\n",
    "        ),\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    file_index = 0\n",
    "    for chunk in client.models.generate_content_stream(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    ):\n",
    "        if (\n",
    "            chunk.candidates is None\n",
    "            or chunk.candidates[0].content is None\n",
    "            or chunk.candidates[0].content.parts is None\n",
    "        ):\n",
    "            continue\n",
    "        if chunk.candidates[0].content.parts[0].inline_data and chunk.candidates[0].content.parts[0].inline_data.data:\n",
    "            file_name = f\"ENTER_FILE_NAME_{file_index}\"\n",
    "            file_index += 1\n",
    "            inline_data = chunk.candidates[0].content.parts[0].inline_data\n",
    "            data_buffer = inline_data.data\n",
    "            file_extension = mimetypes.guess_extension(inline_data.mime_type)\n",
    "            save_binary_file(f\"{file_name}{file_extension}\", data_buffer)\n",
    "        else:\n",
    "            print(chunk.text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
