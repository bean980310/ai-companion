{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6455ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv, get_key, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32da5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"]\n",
    "\n",
    "llm = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    exclude_type = all(k not in model_id.lower() for k in [\"image\", \"realtime\", \"tts\", \"audio\", \"transcribe\", \"codex\", \"search\", \"preview\"])\n",
    "    exclude_model = all(k not in model_id.lower() for k in [\"gpt-4.1-mini\", \"gpt-4.1-nano\", 'gpt-4o-mini', 'chatgpt-4o-latest'])\n",
    "    latest_or_date = any(k in model_id.lower() for k in [\"latest\", \"oss\"]) or bool(date_pattern.search(model_id.lower()))\n",
    "    if include and exclude_type and exclude_model and latest_or_date:\n",
    "        llm.append(model_id)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ef44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(api_key=get_key(\".env\", \"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "llm = []\n",
    "models = client.models.list()\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "    exclude = \"claude-3\" not in model_id.lower()\n",
    "\n",
    "    if exclude:\n",
    "        llm.append(model_id)\n",
    "        \n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "llm = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"gemini\", \"gemma\"])\n",
    "    exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "\n",
    "    if \"generateContent\" in m.supported_actions and include and exclude_type and exclude_model and preview_check:\n",
    "        llm.append(m.name)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26bedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import perplexity\n",
    "from perplexity import Perplexity\n",
    "\n",
    "client = Perplexity(api_key=get_key(\".env\", \"PERPLEXITY_API_KEY\"))\n",
    "\n",
    "llm = []\n",
    "\n",
    "# models = client.chat.completions.create(messages='hi',model='sonar-reasoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xai_sdk\n",
    "\n",
    "client = xai_sdk.Client(api_key=get_key(\".env\", \"XAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be148fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistralai\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=get_key(\".env\", \"MISTRAL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75ce2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta-llama/Llama-4-Scout-17B-16E-Instruct',\n",
       " 'meta-llama/Llama-4-Maverick-17B-128E-Instruct',\n",
       " 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Instruct',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Instruct',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Thinking',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Thinking',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Instruct-FP8',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Thinking-FP8',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Instruct-FP8',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Thinking-FP8',\n",
       " 'zai-org/GLM-4.7',\n",
       " 'zai-org/GLM-4.7-FP8',\n",
       " 'zai-org/GLM-4.6V',\n",
       " 'zai-org/GLM-4.6V-Flash',\n",
       " 'zai-org/GLM-4.5-Air',\n",
       " 'zai-org/GLM-4.6',\n",
       " 'zai-org/GLM-4.5',\n",
       " 'zai-org/GLM-4.5-Air-FP8',\n",
       " 'zai-org/GLM-4.6V-FP8',\n",
       " 'zai-org/GLM-4.5-FP8',\n",
       " 'zai-org/GLM-4.5V-FP8',\n",
       " 'zai-org/GLM-4.5V',\n",
       " 'zai-org/GLM-4.6-FP8']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, InferenceClient\n",
    "\n",
    "client = HfApi(token=get_key(\".env\", \"HF_TOKEN\"), library_name=\"transformers\")\n",
    "\n",
    "# tags = ['image-text-to-text', 'text-generation', 'any-to-any']\n",
    "\n",
    "models = client.list_models(author='meta-llama', search='llama-4', filter='transformers')\n",
    "\n",
    "llm = []\n",
    "\n",
    "# tags = ['image-text-to-text', 'text-generation', 'any-to-any']\n",
    "\n",
    "for m in models:\n",
    "    if \"instruct\" in m.id.lower():\n",
    "        llm.append(m.id)\n",
    "\n",
    "models = client.list_models(author='Qwen', search='Qwen3-VL', filter='transformers')\n",
    "\n",
    "for m in models:\n",
    "    if any(k in m.id.lower() for k in [\"30b-a3b\", \"235b-a22b\"]):\n",
    "        llm.append(m.id)\n",
    "\n",
    "models = client.list_models(author='zai-org', filter='transformers')\n",
    "\n",
    "for m in models:\n",
    "    if any(k in m.id.lower() for k in [\"glm-4.6\", 'glm-4.5', 'glm-4.7']) and \"base\" not in m.id.lower():\n",
    "        llm.append(m.id)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "client = lms.Client(api_host=\"localhost:1234\")\n",
    "downloaded_llm = client.list_downloaded_models(\"llm\")\n",
    "\n",
    "llm = []\n",
    "\n",
    "for m in downloaded_llm:\n",
    "    llm.append(m.model_key)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "llm = []\n",
    "\n",
    "client = ollama.Client(host=\"http://localhost:11434\")\n",
    "models = client.list().models\n",
    "\n",
    "for m in models:\n",
    "    llm.append(m.model)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc9ad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatgpt-image-latest', 'gpt-image-1', 'gpt-image-1-mini', 'gpt-image-1.5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"gpt-image\"]\n",
    "\n",
    "image_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        image_models.append(model_id)\n",
    "\n",
    "image_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6eb2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gemini-2.0-flash-exp-image-generation',\n",
       " 'models/gemini-2.5-flash-image-preview',\n",
       " 'models/gemini-2.5-flash-image',\n",
       " 'models/gemini-3-pro-image-preview',\n",
       " 'models/imagen-4.0-generate-preview-06-06',\n",
       " 'models/imagen-4.0-ultra-generate-preview-06-06',\n",
       " 'models/imagen-4.0-generate-001',\n",
       " 'models/imagen-4.0-ultra-generate-001',\n",
       " 'models/imagen-4.0-fast-generate-001']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "image_models = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"image\"])\n",
    "    # exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    # exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    # preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "\n",
    "    if include:\n",
    "        image_models.append(m.name)\n",
    "\n",
    "image_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comfy_sdk import ComfyUI\n",
    "\n",
    "client=ComfyUI(port=8000)\n",
    "\n",
    "image_models = client.models.list('checkpoints')\n",
    "image_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd42a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "deepl_client = deepl.DeepLClient(auth_key=get_key('.env', 'DEEPL_AUTH_TOKEN'))\n",
    "\n",
    "translator = deepl_client.translate_text(\"로컬 머신을 위한 AI 컴패니언은 UNIX/Linux 커널의 OS에 최적화 되어있습니다. Windows에서도 사용은 가능하지만 Windows에서 직접 실행할경우 GPU 가속을 사용할수 없습니다. Windows에서의 로컬 머신을 위한 AI 컴패니언을 제대로 사용하기 위해 WSL2 환경에서의 사용을 권장합니다.\", target_lang=\"ZH-HANS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e32193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本地机器的AI伴侣已针对UNIX/Linux内核的操作系统进行优化。虽然可在Windows环境下使用，但直接在Windows上运行时无法启用GPU加速功能。为确保在Windows系统中正确使用本地机器的AI伴侣，建议在WSL2环境下运行。\n"
     ]
    }
   ],
   "source": [
    "print(translator.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c1d83a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o-transcribe',\n",
       " 'gpt-4o-mini-transcribe',\n",
       " 'gpt-4o-transcribe-diarize',\n",
       " 'gpt-4o-mini-transcribe-2025-12-15',\n",
       " 'gpt-4o-mini-transcribe-2025-03-20',\n",
       " 'whisper-1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"transcribe\", \"whisper\"]\n",
    "\n",
    "asr_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        asr_models.append(model_id)\n",
    "\n",
    "asr_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ca91519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o-mini-tts-2025-03-20',\n",
       " 'gpt-4o-mini-tts-2025-12-15',\n",
       " 'tts-1-hd',\n",
       " 'tts-1-1106',\n",
       " 'tts-1-hd-1106',\n",
       " 'gpt-4o-mini-tts',\n",
       " 'tts-1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"tts\"]\n",
    "\n",
    "tts_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        tts_models.append(model_id)\n",
    "\n",
    "tts_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b33fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-audio-mini-2025-12-15',\n",
       " 'gpt-4o-audio-preview',\n",
       " 'gpt-4o-audio-preview-2024-12-17',\n",
       " 'gpt-4o-mini-audio-preview-2024-12-17',\n",
       " 'gpt-4o-mini-audio-preview',\n",
       " 'gpt-4o-audio-preview-2025-06-03',\n",
       " 'gpt-audio-2025-08-28',\n",
       " 'gpt-audio',\n",
       " 'gpt-audio-mini',\n",
       " 'gpt-audio-mini-2025-10-06']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"audio\"]\n",
    "\n",
    "audio_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        audio_models.append(model_id)\n",
    "\n",
    "audio_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6911e041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-realtime-mini-2025-12-15',\n",
       " 'gpt-4o-realtime-preview',\n",
       " 'gpt-4o-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-mini-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-mini-realtime-preview',\n",
       " 'gpt-4o-realtime-preview-2025-06-03',\n",
       " 'gpt-realtime',\n",
       " 'gpt-realtime-2025-08-28',\n",
       " 'gpt-realtime-mini',\n",
       " 'gpt-realtime-mini-2025-10-06']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"realtime\"]\n",
    "\n",
    "realtime_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        realtime_models.append(model_id)\n",
    "\n",
    "realtime_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecd15a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "tts_models = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"tts\"])\n",
    "    # exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    # exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    # preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "\n",
    "    if include:\n",
    "        tts_models.append(m.name)\n",
    "\n",
    "tts_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9899bd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/embedding-gecko-001',\n",
       " 'models/gemini-2.5-flash',\n",
       " 'models/gemini-2.5-pro',\n",
       " 'models/gemini-2.0-flash-exp',\n",
       " 'models/gemini-2.0-flash',\n",
       " 'models/gemini-2.0-flash-001',\n",
       " 'models/gemini-2.0-flash-exp-image-generation',\n",
       " 'models/gemini-2.0-flash-lite-001',\n",
       " 'models/gemini-2.0-flash-lite',\n",
       " 'models/gemini-2.0-flash-lite-preview-02-05',\n",
       " 'models/gemini-2.0-flash-lite-preview',\n",
       " 'models/gemini-exp-1206',\n",
       " 'models/gemini-2.5-flash-preview-tts',\n",
       " 'models/gemini-2.5-pro-preview-tts',\n",
       " 'models/gemma-3-1b-it',\n",
       " 'models/gemma-3-4b-it',\n",
       " 'models/gemma-3-12b-it',\n",
       " 'models/gemma-3-27b-it',\n",
       " 'models/gemma-3n-e4b-it',\n",
       " 'models/gemma-3n-e2b-it',\n",
       " 'models/gemini-flash-latest',\n",
       " 'models/gemini-flash-lite-latest',\n",
       " 'models/gemini-pro-latest',\n",
       " 'models/gemini-2.5-flash-lite',\n",
       " 'models/gemini-2.5-flash-image-preview',\n",
       " 'models/gemini-2.5-flash-image',\n",
       " 'models/gemini-2.5-flash-preview-09-2025',\n",
       " 'models/gemini-2.5-flash-lite-preview-09-2025',\n",
       " 'models/gemini-3-pro-preview',\n",
       " 'models/gemini-3-flash-preview',\n",
       " 'models/gemini-3-pro-image-preview',\n",
       " 'models/nano-banana-pro-preview',\n",
       " 'models/gemini-robotics-er-1.5-preview',\n",
       " 'models/gemini-2.5-computer-use-preview-10-2025',\n",
       " 'models/deep-research-pro-preview-12-2025',\n",
       " 'models/embedding-001',\n",
       " 'models/text-embedding-004',\n",
       " 'models/gemini-embedding-exp-03-07',\n",
       " 'models/gemini-embedding-exp',\n",
       " 'models/gemini-embedding-001',\n",
       " 'models/aqa',\n",
       " 'models/imagen-4.0-generate-preview-06-06',\n",
       " 'models/imagen-4.0-ultra-generate-preview-06-06',\n",
       " 'models/imagen-4.0-generate-001',\n",
       " 'models/imagen-4.0-ultra-generate-001',\n",
       " 'models/imagen-4.0-fast-generate-001',\n",
       " 'models/veo-2.0-generate-001',\n",
       " 'models/veo-3.0-generate-001',\n",
       " 'models/veo-3.0-fast-generate-001',\n",
       " 'models/veo-3.1-generate-preview']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "audio_models = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"audio\"])\n",
    "    # exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    # exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    # preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "    audio_models.append(m.name)\n",
    "    # if include:\n",
    "    #     audio_models.append(m.name)\n",
    "\n",
    "audio_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "701caad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "result = client.images.generate(\n",
    "    model=\"gpt-image-1.5\",\n",
    "    prompt=\"\"\"\n",
    "    Anime style, sfw, safe, Minami Asuka (Original Character), an adult tomboy girl, including very boyish handsome shortcut red hair with spiked hair, very handsome face, and heterochromia with blue eyes and yellow eyes. She has feminine body, perfect female body, busty, glamorous, curvy, and voluptuous waist.\n",
    "    She is wearing jirai kei uniform, including black choker, pink shirt, black miniskirt, black thighhighs, and black loafers.\n",
    "    She is standing with legs together, Her hands on own face. She does crazy smile look like a yandere.\n",
    "    Inside the room with the lights off.\n",
    "    \"\"\",\n",
    "    size=\"1024x1536\",\n",
    "    moderation='low',\n",
    "    background=\"transparent\",\n",
    "    quality=\"high\",\n",
    ")\n",
    "\n",
    "image_base64 = result.data[0].b64_json\n",
    "image_bytes = base64.b64decode(image_base64)\n",
    "\n",
    "# Save the image to a file\n",
    "with open(\"gpt_image_example.png\", \"wb\") as f:\n",
    "    f.write(image_bytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
