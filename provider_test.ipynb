{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6455ec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from dotenv import load_dotenv, get_key, set_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32da5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o-2024-05-13',\n",
       " 'gpt-4o-2024-08-06',\n",
       " 'gpt-4o-2024-11-20',\n",
       " 'gpt-4.1-2025-04-14',\n",
       " 'gpt-5-chat-latest',\n",
       " 'gpt-5-2025-08-07',\n",
       " 'gpt-5-mini-2025-08-07',\n",
       " 'gpt-5-nano-2025-08-07',\n",
       " 'gpt-5-pro-2025-10-06',\n",
       " 'gpt-5.1-chat-latest',\n",
       " 'gpt-5.1-2025-11-13',\n",
       " 'gpt-5.2-2025-12-11',\n",
       " 'gpt-5.2-pro-2025-12-11',\n",
       " 'gpt-5.2-chat-latest']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"gpt-4o\", \"gpt-4.1\", \"gpt-5\", \"gpt-oss\"]\n",
    "\n",
    "llm = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    date_pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\")\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    exclude_type = all(k not in model_id.lower() for k in [\"image\", \"realtime\", \"tts\", \"audio\", \"transcribe\", \"codex\", \"search\", \"preview\"])\n",
    "    exclude_model = all(k not in model_id.lower() for k in [\"gpt-4.1-mini\", \"gpt-4.1-nano\", 'gpt-4o-mini', 'chatgpt-4o-latest'])\n",
    "    latest_or_date = any(k in model_id.lower() for k in [\"latest\", \"oss\"]) or bool(date_pattern.search(model_id.lower()))\n",
    "    if include and exclude_type and exclude_model and latest_or_date:\n",
    "        llm.append(model_id)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4ef44f",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpcore/_sync/connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpcore/_sync/connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpcore/_sync/connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpcore/_backends/sync.py:207\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    208\u001b[39m     sock = socket.create_connection(\n\u001b[32m    209\u001b[39m         address,\n\u001b[32m    210\u001b[39m         timeout,\n\u001b[32m    211\u001b[39m         source_address=source_address,\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/anthropic/_base_client.py:1072\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_transports/default.py:249\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m    250\u001b[39m     resp = \u001b[38;5;28mself\u001b[39m._pool.handle_request(req)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m client = Anthropic(api_key=get_key(\u001b[33m\"\u001b[39m\u001b[33m.env\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mANTHROPIC_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      6\u001b[39m llm = []\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m models = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m models.data:\n\u001b[32m     10\u001b[39m     model_id = m.id\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/anthropic/resources/models.py:134\u001b[39m, in \u001b[36mModels.list\u001b[39m\u001b[34m(self, after_id, before_id, limit, betas, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03mList available models.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    128\u001b[39m \u001b[33;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m extra_headers = {\n\u001b[32m    131\u001b[39m     **strip_not_given({\u001b[33m\"\u001b[39m\u001b[33manthropic-beta\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m betas) \u001b[38;5;28;01mif\u001b[39;00m is_given(betas) \u001b[38;5;28;01melse\u001b[39;00m not_given}),\n\u001b[32m    132\u001b[39m     **(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    133\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/models\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSyncPage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mModelInfo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mafter_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mafter_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbefore_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbefore_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlimit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_list_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mModelListParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mModelInfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/anthropic/_base_client.py:1450\u001b[39m, in \u001b[36mSyncAPIClient.get_api_list\u001b[39m\u001b[34m(self, path, model, page, body, options, method)\u001b[39m\n\u001b[32m   1439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_api_list\u001b[39m(\n\u001b[32m   1440\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1441\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1447\u001b[39m     method: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mget\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1448\u001b[39m ) -> SyncPageT:\n\u001b[32m   1449\u001b[39m     opts = FinalRequestOptions.construct(method=method, url=path, json_data=body, **options)\n\u001b[32m-> \u001b[39m\u001b[32m1450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_api_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/anthropic/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient._request_api_list\u001b[39m\u001b[34m(self, model, page, options)\u001b[39m\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\n\u001b[32m   1247\u001b[39m options.post_parser = _parser\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/ai/lib/python3.12/site-packages/anthropic/_base_client.py:1104\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1103\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1106\u001b[39m log.debug(\n\u001b[32m   1107\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1108\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1112\u001b[39m     response.headers,\n\u001b[32m   1113\u001b[39m )\n\u001b[32m   1114\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mrequest-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(api_key=get_key(\".env\", \"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "llm = []\n",
    "models = client.models.list()\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "    exclude = \"claude-3\" not in model_id.lower()\n",
    "\n",
    "    if exclude:\n",
    "        llm.append(model_id)\n",
    "        \n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "llm = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"gemini\", \"gemma\"])\n",
    "    exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "\n",
    "    if \"generateContent\" in m.supported_actions and include and exclude_type and exclude_model and preview_check:\n",
    "        llm.append(m.name)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26bedbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import perplexity\n",
    "from perplexity import Perplexity\n",
    "\n",
    "client = Perplexity(api_key=get_key(\".env\", \"PERPLEXITY_API_KEY\"))\n",
    "\n",
    "llm = []\n",
    "\n",
    "# models = client.chat.completions.create(messages='hi',model='sonar-reasoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xai_sdk\n",
    "\n",
    "client = xai_sdk.Client(api_key=get_key(\".env\", \"XAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be148fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mistralai\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=get_key(\".env\", \"MISTRAL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "75ce2989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meta-llama/Llama-4-Scout-17B-16E-Instruct',\n",
       " 'meta-llama/Llama-4-Maverick-17B-128E-Instruct',\n",
       " 'meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Instruct',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Instruct',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Thinking',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Thinking',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Instruct-FP8',\n",
       " 'Qwen/Qwen3-VL-30B-A3B-Thinking-FP8',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Instruct-FP8',\n",
       " 'Qwen/Qwen3-VL-235B-A22B-Thinking-FP8',\n",
       " 'zai-org/GLM-4.7',\n",
       " 'zai-org/GLM-4.7-FP8',\n",
       " 'zai-org/GLM-4.6V',\n",
       " 'zai-org/GLM-4.6V-Flash',\n",
       " 'zai-org/GLM-4.5-Air',\n",
       " 'zai-org/GLM-4.6',\n",
       " 'zai-org/GLM-4.5',\n",
       " 'zai-org/GLM-4.5-Air-FP8',\n",
       " 'zai-org/GLM-4.6V-FP8',\n",
       " 'zai-org/GLM-4.5-FP8',\n",
       " 'zai-org/GLM-4.5V-FP8',\n",
       " 'zai-org/GLM-4.5V',\n",
       " 'zai-org/GLM-4.6-FP8']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, InferenceClient\n",
    "\n",
    "client = HfApi(token=get_key(\".env\", \"HF_TOKEN\"), library_name=\"transformers\")\n",
    "\n",
    "# tags = ['image-text-to-text', 'text-generation', 'any-to-any']\n",
    "\n",
    "models = client.list_models(author='meta-llama', search='llama-4', filter='transformers')\n",
    "\n",
    "llm = []\n",
    "\n",
    "# tags = ['image-text-to-text', 'text-generation', 'any-to-any']\n",
    "\n",
    "for m in models:\n",
    "    if \"instruct\" in m.id.lower():\n",
    "        llm.append(m.id)\n",
    "\n",
    "models = client.list_models(author='Qwen', search='Qwen3-VL', filter='transformers')\n",
    "\n",
    "for m in models:\n",
    "    if any(k in m.id.lower() for k in [\"30b-a3b\", \"235b-a22b\"]):\n",
    "        llm.append(m.id)\n",
    "\n",
    "models = client.list_models(author='zai-org', filter='transformers')\n",
    "\n",
    "for m in models:\n",
    "    if any(k in m.id.lower() for k in [\"glm-4.6\", 'glm-4.5', 'glm-4.7']) and \"base\" not in m.id.lower():\n",
    "        llm.append(m.id)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "\n",
    "client = lms.Client(api_host=\"localhost:1234\")\n",
    "downloaded_llm = client.list_downloaded_models(\"llm\")\n",
    "\n",
    "llm = []\n",
    "\n",
    "for m in downloaded_llm:\n",
    "    llm.append(m.model_key)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "llm = []\n",
    "\n",
    "client = ollama.Client(host=\"http://localhost:11434\")\n",
    "models = client.list().models\n",
    "\n",
    "for m in models:\n",
    "    llm.append(m.model)\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dc9ad45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chatgpt-image-latest', 'gpt-image-1', 'gpt-image-1-mini', 'gpt-image-1.5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"gpt-image\"]\n",
    "\n",
    "image_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        image_models.append(model_id)\n",
    "\n",
    "image_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6eb2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gemini-2.0-flash-exp-image-generation',\n",
       " 'models/gemini-2.5-flash-image-preview',\n",
       " 'models/gemini-2.5-flash-image',\n",
       " 'models/gemini-3-pro-image-preview',\n",
       " 'models/imagen-4.0-generate-preview-06-06',\n",
       " 'models/imagen-4.0-ultra-generate-preview-06-06',\n",
       " 'models/imagen-4.0-generate-001',\n",
       " 'models/imagen-4.0-ultra-generate-001',\n",
       " 'models/imagen-4.0-fast-generate-001']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "image_models = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"image\"])\n",
    "    # exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    # exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    # preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "\n",
    "    if include:\n",
    "        image_models.append(m.name)\n",
    "\n",
    "image_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comfy_sdk import ComfyUI\n",
    "\n",
    "client=ComfyUI(port=8000)\n",
    "\n",
    "image_models = client.models.list('checkpoints')\n",
    "image_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd42a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "deepl_client = deepl.DeepLClient(auth_key=get_key('.env', 'DEEPL_AUTH_TOKEN'))\n",
    "\n",
    "translator = deepl_client.translate_text(\"로컬 머신을 위한 AI 컴패니언은 UNIX/Linux 커널의 OS에 최적화 되어있습니다. Windows에서도 사용은 가능하지만 Windows에서 직접 실행할경우 GPU 가속을 사용할수 없습니다. Windows에서의 로컬 머신을 위한 AI 컴패니언을 제대로 사용하기 위해 WSL2 환경에서의 사용을 권장합니다.\", target_lang=\"ZH-HANS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e32193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本地机器的AI伴侣已针对UNIX/Linux内核的操作系统进行优化。虽然可在Windows环境下使用，但直接在Windows上运行时无法启用GPU加速功能。为确保在Windows系统中正确使用本地机器的AI伴侣，建议在WSL2环境下运行。\n"
     ]
    }
   ],
   "source": [
    "print(translator.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c1d83a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o-transcribe',\n",
       " 'gpt-4o-mini-transcribe',\n",
       " 'gpt-4o-transcribe-diarize',\n",
       " 'gpt-4o-mini-transcribe-2025-12-15',\n",
       " 'gpt-4o-mini-transcribe-2025-03-20',\n",
       " 'whisper-1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"transcribe\", \"whisper\"]\n",
    "\n",
    "asr_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        asr_models.append(model_id)\n",
    "\n",
    "asr_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ca91519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o-mini-tts-2025-03-20',\n",
       " 'gpt-4o-mini-tts-2025-12-15',\n",
       " 'tts-1-hd',\n",
       " 'tts-1-1106',\n",
       " 'tts-1-hd-1106',\n",
       " 'gpt-4o-mini-tts',\n",
       " 'tts-1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"tts\"]\n",
    "\n",
    "tts_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        tts_models.append(model_id)\n",
    "\n",
    "tts_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86b33fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-audio-mini-2025-12-15',\n",
       " 'gpt-4o-audio-preview',\n",
       " 'gpt-4o-audio-preview-2024-12-17',\n",
       " 'gpt-4o-mini-audio-preview-2024-12-17',\n",
       " 'gpt-4o-mini-audio-preview',\n",
       " 'gpt-4o-audio-preview-2025-06-03',\n",
       " 'gpt-audio-2025-08-28',\n",
       " 'gpt-audio',\n",
       " 'gpt-audio-mini',\n",
       " 'gpt-audio-mini-2025-10-06']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"audio\"]\n",
    "\n",
    "audio_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        audio_models.append(model_id)\n",
    "\n",
    "audio_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6911e041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-realtime-mini-2025-12-15',\n",
       " 'gpt-4o-realtime-preview',\n",
       " 'gpt-4o-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-mini-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-mini-realtime-preview',\n",
       " 'gpt-4o-realtime-preview-2025-06-03',\n",
       " 'gpt-realtime',\n",
       " 'gpt-realtime-2025-08-28',\n",
       " 'gpt-realtime-mini',\n",
       " 'gpt-realtime-mini-2025-10-06']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "gpt_pattern = [\"realtime\"]\n",
    "\n",
    "realtime_models = []\n",
    "\n",
    "for m in models.data:\n",
    "    model_id = m.id\n",
    "\n",
    "    include = any(k in model_id.lower() for k in gpt_pattern)\n",
    "    if include:\n",
    "        realtime_models.append(model_id)\n",
    "\n",
    "realtime_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecd15a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gemini-2.5-flash-preview-tts', 'models/gemini-2.5-pro-preview-tts']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "tts_models = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"tts\"])\n",
    "    # exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    # exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    # preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "\n",
    "    if include:\n",
    "        tts_models.append(m.name)\n",
    "\n",
    "tts_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9899bd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/embedding-gecko-001',\n",
       " 'models/gemini-2.5-flash',\n",
       " 'models/gemini-2.5-pro',\n",
       " 'models/gemini-2.0-flash-exp',\n",
       " 'models/gemini-2.0-flash',\n",
       " 'models/gemini-2.0-flash-001',\n",
       " 'models/gemini-2.0-flash-exp-image-generation',\n",
       " 'models/gemini-2.0-flash-lite-001',\n",
       " 'models/gemini-2.0-flash-lite',\n",
       " 'models/gemini-2.0-flash-lite-preview-02-05',\n",
       " 'models/gemini-2.0-flash-lite-preview',\n",
       " 'models/gemini-exp-1206',\n",
       " 'models/gemini-2.5-flash-preview-tts',\n",
       " 'models/gemini-2.5-pro-preview-tts',\n",
       " 'models/gemma-3-1b-it',\n",
       " 'models/gemma-3-4b-it',\n",
       " 'models/gemma-3-12b-it',\n",
       " 'models/gemma-3-27b-it',\n",
       " 'models/gemma-3n-e4b-it',\n",
       " 'models/gemma-3n-e2b-it',\n",
       " 'models/gemini-flash-latest',\n",
       " 'models/gemini-flash-lite-latest',\n",
       " 'models/gemini-pro-latest',\n",
       " 'models/gemini-2.5-flash-lite',\n",
       " 'models/gemini-2.5-flash-image-preview',\n",
       " 'models/gemini-2.5-flash-image',\n",
       " 'models/gemini-2.5-flash-preview-09-2025',\n",
       " 'models/gemini-2.5-flash-lite-preview-09-2025',\n",
       " 'models/gemini-3-pro-preview',\n",
       " 'models/gemini-3-flash-preview',\n",
       " 'models/gemini-3-pro-image-preview',\n",
       " 'models/nano-banana-pro-preview',\n",
       " 'models/gemini-robotics-er-1.5-preview',\n",
       " 'models/gemini-2.5-computer-use-preview-10-2025',\n",
       " 'models/deep-research-pro-preview-12-2025',\n",
       " 'models/embedding-001',\n",
       " 'models/text-embedding-004',\n",
       " 'models/gemini-embedding-exp-03-07',\n",
       " 'models/gemini-embedding-exp',\n",
       " 'models/gemini-embedding-001',\n",
       " 'models/aqa',\n",
       " 'models/imagen-4.0-generate-preview-06-06',\n",
       " 'models/imagen-4.0-ultra-generate-preview-06-06',\n",
       " 'models/imagen-4.0-generate-001',\n",
       " 'models/imagen-4.0-ultra-generate-001',\n",
       " 'models/imagen-4.0-fast-generate-001',\n",
       " 'models/veo-2.0-generate-001',\n",
       " 'models/veo-3.0-generate-001',\n",
       " 'models/veo-3.0-fast-generate-001',\n",
       " 'models/veo-3.1-generate-preview']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=get_key(\".env\", \"GEMINI_API_KEY\"))\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "audio_models = []\n",
    "\n",
    "for m in models.page:\n",
    "    include = any(k in m.name.lower() for k in [\"audio\"])\n",
    "    # exclude_type = all(k not in m.name.lower() for k in [\"embedding\", \"tts\", \"exp\"])\n",
    "    # exclude_model = all(k not in m.name.lower() for k in [\"gemini-2.0\"])\n",
    "\n",
    "    # preview_check = (\"preview\" not in m.name.lower()) or any(k in m.name.lower() for k in [\"gemini-3\"])\n",
    "    audio_models.append(m.name)\n",
    "    # if include:\n",
    "    #     audio_models.append(m.name)\n",
    "\n",
    "audio_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "701caad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "client = OpenAI(api_key=get_key('.env', 'OPENAI_API_KEY'))\n",
    "\n",
    "result = client.images.generate(\n",
    "    model=\"gpt-image-1.5\",\n",
    "    prompt=\"\"\"\n",
    "    Anime style, sfw, safe, Minami Asuka (Original Character), an adult tomboy girl, including very boyish handsome shortcut red hair with spiked hair, very handsome face, and heterochromia with blue eyes and yellow eyes. She has feminine body, perfect female body, busty, glamorous, curvy, and voluptuous waist.\n",
    "    She is wearing jirai kei uniform, including black choker, pink shirt, black miniskirt, black thighhighs, and black loafers.\n",
    "    She is standing with legs together, Her hands on own face. She does crazy smile look like a yandere.\n",
    "    Inside the room with the lights off.\n",
    "    \"\"\",\n",
    "    size=\"1024x1536\",\n",
    "    moderation='low',\n",
    "    background=\"transparent\",\n",
    "    quality=\"high\",\n",
    ")\n",
    "\n",
    "image_base64 = result.data[0].b64_json\n",
    "image_bytes = base64.b64decode(image_base64)\n",
    "\n",
    "# Save the image to a file\n",
    "with open(\"gpt_image_example.png\", \"wb\") as f:\n",
    "    f.write(image_bytes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
